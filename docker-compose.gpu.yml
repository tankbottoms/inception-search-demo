version: '3.8'

services:
  gpu-inference:
    build:
      context: ./python-gpu-service
      dockerfile: Dockerfile
    container_name: inception-gpu
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - MODEL_CACHE_DIR=/models
      - PORT=8006
      - PROCESSING_BATCH_SIZE=32
      - MAX_BATCH_SIZE=100
    volumes:
      - ./models:/models:rw
    ports:
      - "8006:8006"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # TypeScript CPU service (for comparison)
  cpu-inference:
    build:
      context: .
      dockerfile: demo/Dockerfile
    container_name: inception-cpu
    environment:
      - PORT=8005
      - MODEL_CACHE_DIR=/models
    volumes:
      - ./models:/models:ro
    ports:
      - "8005:8005"
    profiles:
      - comparison
