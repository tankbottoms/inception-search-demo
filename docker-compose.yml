# Inception ONNX - Docker Compose Configuration
# Multi-platform inference service with CPU and GPU support

services:
  # TypeScript backend - CPU (Apple Silicon, generic ARM64)
  backend-cpu:
    container_name: inception-cpu
    build:
      context: .
      dockerfile: Dockerfile
    profiles: ["cpu", "default", "demo"]
    environment:
      - PORT=8005
      - EXECUTION_PROVIDER=cpu
      - MODEL_REGISTRY=/models/registry.json
      - CONVERTER_URL=http://converter:8010
      - LOG_LEVEL=info
      - ENABLE_METRICS=true
    env_file:
      - .env
    volumes:
      - ./models:/models
    ports:
      - "8005:8005"
    networks:
      default:
        aliases:
          - inception
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # TypeScript backend - GPU (DGX Spark, NVIDIA CUDA)
  backend-gpu:
    container_name: inception-gpu
    build:
      context: .
      dockerfile: Dockerfile.cuda
    profiles: ["gpu", "cuda", "spark"]
    environment:
      - PORT=8005
      - EXECUTION_PROVIDER=cuda
      - MODEL_REGISTRY=/models/registry.json
      - CONVERTER_URL=http://converter:8010
      - LOG_LEVEL=info
      - ENABLE_METRICS=true
    env_file:
      - .env
    volumes:
      - ./models:/models
    ports:
      - "8005:8005"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      default:
        aliases:
          - inception
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Python ONNX conversion service (auto-converts on startup)
  converter:
    container_name: inception-converter
    build:
      context: ./converter
    profiles: ["convert", "full"]
    environment:
      - PORT=8010
      - MODELS_DIR=/models
      - AUTO_CONVERT=true
      - HF_HOME=/models/.cache/huggingface
    volumes:
      - ./models:/models
    ports:
      - "8010:8010"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Demo client
  demo:
    container_name: inception-demo
    build:
      context: ./demo
    profiles: ["demo"]
    depends_on:
      backend-cpu:
        condition: service_healthy
    environment:
      - INCEPTION_URL=http://inception:8005
    env_file:
      - .env
    volumes:
      - ./demo/files:/app/files
      - ./demo/output:/app/output
      - ./demo/logs:/app/logs
    networks:
      - default

  # Legacy Python backend (for comparison)
  inception-legacy:
    container_name: inception-legacy
    build:
      context: ./backup/inception
      args:
        TARGET_ENV: dev
        BASE_IMAGE: ubuntu:24.04
        UV_SYNC_FLAGS: "-v"
    profiles: ["legacy", "python"]
    environment:
      - TRANSFORMER_MODEL_NAME=freelawproject/modernbert-embed-base_finetune_512
      - MAX_BATCH_SIZE=32
    ports:
      - "8006:8005"

networks:
  default:
    driver: bridge
