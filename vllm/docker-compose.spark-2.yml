# vLLM Distributed Inference - Spark-2 (Worker Node)
# Ray worker node

services:
  vllm:
    container_name: vllm-worker
    image: nvcr.io/nvidia/vllm:25.10-py3
    restart: unless-stopped
    entrypoint: "/bin/bash -c \"ray start --block --address=192.168.100.10:6379\""
    shm_size: '10.24g'
    network_mode: host
    volumes:
      - /home/rooot/.cache/huggingface:/root/.cache/huggingface
    environment:
      # Ray cluster configuration
      - MASTER_ADDR=192.168.100.10
      - VLLM_HOST_IP=192.168.100.11
      # Network interfaces for distributed communication
      - UCX_NET_DEVICES=enP2p1s0f1np1
      - NCCL_SOCKET_IFNAME=enP2p1s0f1np1
      - OMPI_MCA_btl_tcp_if_include=enP2p1s0f1np1
      - GLOO_SOCKET_IFNAME=enP2p1s0f1np1
      - TP_SOCKET_IFNAME=enP2p1s0f1np1
      # Memory management
      - RAY_memory_monitor_refresh_ms=0
      # CUDA configuration for DGX Spark
      - TORCH_CUDA_ARCH_LIST=12.1a
      - TRITON_PTXAS_PATH=/usr/local/cuda/bin/ptxas
      - PATH=/usr/local/cuda/bin:$PATH
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
