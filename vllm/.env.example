# vLLM Hydra Cluster Configuration
# Copy this file to .env and update values as needed

# =============================================================================
# CLUSTER CONFIGURATION
# =============================================================================

# Network configuration for Ray cluster (optional)
MASTER_ADDR=192.168.100.10
SPARK1_IP=192.168.100.10
SPARK2_IP=192.168.100.11

# Network interface for high-speed interconnect
NETWORK_INTERFACE=enP2p1s0f1np1

# HuggingFace cache directory
HF_CACHE_DIR=/home/rooot/.cache/huggingface

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Embeddings Model (vllm-freelaw-modernbert) - Core service
EMBEDDINGS_MODEL=freelawproject/modernbert-embed-base_finetune_512
EMBEDDINGS_PORT=8001
EMBEDDINGS_GPU_MEMORY_UTIL=0.10

# HunyuanOCR Model (vllm-hunyuanOCR) - Core service
HUNYUAN_OCR_MODEL=tencent/HunyuanOCR
HUNYUAN_OCR_PORT=8003
HUNYUAN_OCR_GPU_MEMORY_UTIL=0.40

# GPT-OSS 20B Model (vllm-gpt-oss-20b) - Use --profile gpt-oss
GPT_OSS_20B_MODEL=openai/gpt-oss-20b
GPT_OSS_20B_PORT=8004
GPT_OSS_GPU_MEMORY_UTIL=0.45
GPT_OSS_20B_MAX_MODEL_LEN=8192
GPT_OSS_MAX_NUM_SEQS=32

# Llama Model (vllm-llama) - Use --profile llama
LLAMA_MODEL=meta-llama/Llama-3.1-8B-Instruct
LLAMA_GPU_MEMORY_UTIL=0.45
LLAMA_MAX_MODEL_LEN=4096

# =============================================================================
# GPU CONFIGURATION
# =============================================================================

# CUDA architecture (12.1a for Blackwell/GB200)
TORCH_CUDA_ARCH_LIST=12.1a

# Tensor parallelism (set to 2 for distributed across 2 nodes)
TENSOR_PARALLEL_SIZE=1

# =============================================================================
# VLLM SERVER OPTIONS
# =============================================================================

# vLLM Docker image
# NVIDIA 25.12+ recommended for GPT-OSS support
VLLM_IMAGE=nvcr.io/nvidia/vllm:25.12-py3

# vLLM Docker image for Ray cluster
VLLM_RAY_IMAGE=nvcr.io/nvidia/vllm:25.12-py3

# Shared memory size
SHM_SIZE=10.24g

# API key (optional)
VLLM_API_KEY=

# =============================================================================
# MONITOR CONFIGURATION
# =============================================================================

# Monitor check interval (seconds)
MONITOR_INTERVAL=60

# Enable auto-restart on failure
MONITOR_AUTO_RESTART=true

# Log file location
MONITOR_LOG_FILE=/var/log/vllm-hydra/monitor.log

# =============================================================================
# BENCHMARK CONFIGURATION
# =============================================================================

# Test PDF file for benchmarks
BENCHMARK_PDF_FILE=/files/test-sample.pdf

# Number of benchmark iterations
BENCHMARK_ITERATIONS=10

# API URL for client
API_URL=http://localhost:8001
